{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a443fe76-9108-481a-bcc4-096fc679a948",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor (with NaNs):\n",
      "[[[100.75 110.9     nan]\n",
      "  [126.   116.1     nan]]\n",
      "\n",
      " [[ 90.8  105.95    nan]\n",
      "  [141.15 151.25 146.3 ]]]\n",
      "\n",
      "Reconstructed Tensor (Predicted NaNs):\n",
      "[[[100.75000607 110.89999335 118.60554921]\n",
      "  [125.99999959 116.10000045 120.50929949]]\n",
      "\n",
      " [[ 90.79999704 105.95000324 124.62033106]\n",
      "  [141.14999793 151.25000226 146.29999822]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "# Step 1: Create the Initial 3D Tensor (2x2x3) with Missing Values (NaNs)\n",
    "tensor_data = np.array([\n",
    "    [[(1.5 + 200) / 2, (1.8 + 220) / 2, np.nan],  # CPU 100, GPU 50\n",
    "     [(2.0 + 250) / 2, (2.2 + 230) / 2, np.nan]], # CPU 100, GPU 100\n",
    "\n",
    "    [[(1.6 + 180) / 2, (1.9 + 210) / 2, np.nan],  # CPU 200, GPU 50\n",
    "     [(2.3 + 280) / 2, (2.5 + 300) / 2, (2.6 + 290) / 2]]  # CPU 200, GPU 100\n",
    "])\n",
    "\n",
    "# Print the Results\n",
    "print(\"Original Tensor (with NaNs):\")\n",
    "print(tensor_data)\n",
    "\n",
    "# ðŸ”¹ Step 2: Fill NaNs with Initial Estimates (Mean Imputation)\n",
    "nan_mask = np.isnan(tensor_data)  # Identify missing values\n",
    "mean_value = np.nanmean(tensor_data) if np.any(~nan_mask) else 0  # Compute mean\n",
    "tensor_filled = tensor_data.copy()\n",
    "tensor_filled[nan_mask] = mean_value  # Fill NaNs with the mean, but keep the mask\n",
    "\n",
    "# ðŸ”¹ Step 3: Apply CP Tensor Factorization\n",
    "mask = ~nan_mask  # Use only known values for training\n",
    "rank = min(5, tensor_data.shape[2])  # Ensure enough rank for better approximation\n",
    "\n",
    "factors = parafac(tensor_filled, rank=rank, init='svd', mask=mask)  # Use 'svd' initialization\n",
    "\n",
    "# ðŸ”¹ Step 4: Reconstruct the Completed Tensor\n",
    "completed_tensor = tl.cp_to_tensor(factors)\n",
    "\n",
    "# ðŸ”¹ Step 5: Restore Missing Value Positions with CP Predictions\n",
    "tensor_data[nan_mask] = completed_tensor[nan_mask]\n",
    "\n",
    "\n",
    "print(\"\\nReconstructed Tensor (Predicted NaNs):\")\n",
    "print(completed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ddcfb40-4362-43e8-95ce-0bba774e7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, tucker\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statistics import mean \n",
    "\n",
    "def cf():\n",
    "    selected_combinations = [\n",
    "    (120, 110), (120, 250), (140, 110), (140, 250), \n",
    "    (160, 110), (160, 250), (180, 110), (180, 250), \n",
    "    (200, 110), (200, 120), (540, 250)]\n",
    "    \n",
    "    RMSE = []\n",
    "    # Define directories\n",
    "    offline_dir = \"./altis_power_cap_res/offline/\"\n",
    "    online_dir = \"./altis_power_cap_res/online/\"\n",
    "    \n",
    "    # Get all application CSV files from offline data\n",
    "    offline_csv_files = [f for f in os.listdir(offline_dir) if f.endswith(\"_performance.csv\")]\n",
    "    online_csv_files = [f for f in os.listdir(online_dir) if f.endswith(\"_performance.csv\")]\n",
    "    \n",
    "    # Extract application names\n",
    "    offline_apps = sorted([f.replace(\"_performance.csv\", \"\") for f in offline_csv_files])\n",
    "    online_apps = sorted([f.replace(\"_performance.csv\", \"\") for f in online_csv_files])\n",
    "    \n",
    "    # Load offline data\n",
    "    df_list = []\n",
    "    for file in offline_csv_files:\n",
    "        app_name = file.replace(\"_performance.csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(offline_dir, file))\n",
    "        df[\"App\"] = app_name\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df_full = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Step 1: Encode CPU Power, GPU Power, and Applications\n",
    "    cpu_power_levels = sorted(df_full[\"CPU Power Cap\"].unique())\n",
    "    gpu_power_levels = sorted(df_full[\"GPU Power Cap\"].unique())\n",
    "    \n",
    "    cpu_index_map = {power: idx for idx, power in enumerate(cpu_power_levels)}\n",
    "    gpu_index_map = {power: idx for idx, power in enumerate(gpu_power_levels)}\n",
    "    app_index_map = {app: idx for idx, app in enumerate(offline_apps)}\n",
    "    \n",
    "    num_cpu = len(cpu_power_levels)\n",
    "    num_gpu = len(gpu_power_levels)\n",
    "    num_apps = len(offline_apps)\n",
    "    \n",
    "    # Step 2: Initialize 3D Tensor (IPS + FLOPs)/2\n",
    "    tensor_data = np.full((num_cpu, num_gpu, num_apps), np.nan)\n",
    "    \n",
    "    # Populate tensor with offline data\n",
    "    for _, row in df_full.iterrows():\n",
    "        cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "        gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "        app_idx = app_index_map[row[\"App\"]]\n",
    "        tensor_data[cpu_idx, gpu_idx, app_idx] = (row[\"IPS\"] + row[\"FLOPS\"]) / 2\n",
    "    \n",
    "    # Step 3: Normalize Data for Stability\n",
    "    tensor_max = np.nanmax(tensor_data)\n",
    "    tensor_data /= tensor_max  # Scale between [0, 1]\n",
    "    \n",
    "    # Step 4: Process Online Applications\n",
    "    for file in online_csv_files:\n",
    "        app_name = file.replace(\"_performance.csv\", \"\")\n",
    "        df_online = pd.read_csv(os.path.join(online_dir, file))\n",
    "        df_online[\"App\"] = app_name\n",
    "    \n",
    "        # Expand tensor (add new app dimension)\n",
    "        tensor_data = np.pad(tensor_data, ((0, 0), (0, 0), (0, 1)), constant_values=np.nan)\n",
    "        app_index_map[app_name] = tensor_data.shape[2] - 1  # Update app index\n",
    "    \n",
    "        # # Select 20% of rows as known values\n",
    "        df_known = df_online.sample(frac=0.2, random_state=42)\n",
    "        df_test = df_online.drop(df_known.index)\n",
    "        # Fill tensor with known values\n",
    "        for _, row in df_known.iterrows():\n",
    "            cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "            gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "            app_idx = app_index_map[row[\"App\"]]\n",
    "            tensor_data[cpu_idx, gpu_idx, app_idx] = (row[\"IPS\"] + row[\"FLOPS\"]) / 2 / tensor_max\n",
    "\n",
    "        # df_known = df_online[df_online[[\"CPU Power Cap\", \"GPU Power Cap\"]].apply(tuple, axis=1).isin(selected_combinations)]\n",
    "\n",
    "        # # Remaining rows are for testing\n",
    "        # df_test = df_online.drop(df_known.index)\n",
    "        \n",
    "        # # Fill tensor with known values\n",
    "        # for _, row in df_known.iterrows():\n",
    "        #     cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "        #     gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "        #     app_idx = app_index_map[row[\"App\"]]\n",
    "        #     tensor_data[cpu_idx, gpu_idx, app_idx] = (row[\"IPS\"] + row[\"FLOPS\"]) / 2 / tensor_max\n",
    "    \n",
    "        # Step 5: Use SVD-Based Imputation for NaNs\n",
    "        nan_mask = np.isnan(tensor_data)\n",
    "        tensor_2d = tensor_data.reshape(-1, tensor_data.shape[-1])  # Flatten only last axis\n",
    "        \n",
    "        # Fill NaNs with the mean before SVD\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        tensor_2d_filled = imputer.fit_transform(tensor_2d)\n",
    "        \n",
    "        # Apply SVD\n",
    "        svd = TruncatedSVD(n_components=min(3, tensor_2d_filled.shape[1]-1))\n",
    "        low_rank_approx = svd.fit_transform(tensor_2d_filled)\n",
    "        tensor_filled = svd.inverse_transform(low_rank_approx)  # Restores original structure\n",
    "        \n",
    "        # Reshape back to original tensor shape\n",
    "        tensor_filled = tensor_filled.reshape(tensor_data.shape)\n",
    "    \n",
    "        # Step 6: Apply Tucker Decomposition Instead of CP\n",
    "        ranks = [min(10, tensor_data.shape[0]), min(10, tensor_data.shape[1]), min(10, tensor_data.shape[2])]\n",
    "        core, factors = tucker(tensor_filled, rank=ranks)\n",
    "        completed_tensor = tl.tucker_to_tensor((core, factors))\n",
    "    \n",
    "        # Step 7: Evaluate Predictions\n",
    "        test_data = []\n",
    "        for _, row in df_test.iterrows():\n",
    "            cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "            gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "            app_idx = app_index_map[row[\"App\"]]\n",
    "            test_data.append((cpu_idx, gpu_idx, app_idx, (row[\"IPS\"] + row[\"FLOPS\"]) / 2 / tensor_max))\n",
    "    \n",
    "        true_values, predicted_values = [], []\n",
    "        for cpu_idx, gpu_idx, app_idx, true_val in test_data:\n",
    "            predicted_val = completed_tensor[cpu_idx, gpu_idx, app_idx]\n",
    "            true_values.append(true_val)\n",
    "            predicted_values.append(predicted_val)\n",
    "    \n",
    "        # Compute Accuracy Metrics\n",
    "        mae = mean_absolute_error(true_values, predicted_values) * tensor_max  # Rescale values\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values)) * tensor_max\n",
    "        r2 = r2_score(true_values, predicted_values)\n",
    "        RMSE.append(rmse)\n",
    "    \n",
    "        # print(f\"Prediction Accuracy for {app_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, RÂ²={r2:.4f}\")\n",
    "    return round(float(mean(RMSE)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e71f9e86-526f-4f2b-b8f6-683a48c29e4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statistics import mean\n",
    "\n",
    "def cf():\n",
    "    selected_combinations = [\n",
    "        (120, 110), (120, 250), (140, 110), (140, 250), \n",
    "        (160, 110), (160, 250), (180, 110), (180, 250), \n",
    "        (200, 110), (200, 120), (540, 250)\n",
    "    ]\n",
    "    \n",
    "    RMSE_IPS = []\n",
    "    RMSE_FLOPS = []\n",
    "    \n",
    "    offline_dir = \"./altis_power_cap_res/offline/\"\n",
    "    online_dir = \"./altis_power_cap_res/online/\"\n",
    "    \n",
    "    offline_csv_files = [f for f in os.listdir(offline_dir) if f.endswith(\"_performance.csv\")]\n",
    "    online_csv_files = [f for f in os.listdir(online_dir) if f.endswith(\"_performance.csv\")]\n",
    "    \n",
    "    offline_apps = sorted([f.replace(\"_performance.csv\", \"\") for f in offline_csv_files])\n",
    "    online_apps = sorted([f.replace(\"_performance.csv\", \"\") for f in online_csv_files])\n",
    "    \n",
    "    df_list = []\n",
    "    for file in offline_csv_files:\n",
    "        app_name = file.replace(\"_performance.csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(offline_dir, file))\n",
    "        df[\"App\"] = app_name\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df_full = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    cpu_power_levels = sorted(df_full[\"CPU Power Cap\"].unique())\n",
    "    gpu_power_levels = sorted(df_full[\"GPU Power Cap\"].unique())\n",
    "    \n",
    "    cpu_index_map = {power: idx for idx, power in enumerate(cpu_power_levels)}\n",
    "    gpu_index_map = {power: idx for idx, power in enumerate(gpu_power_levels)}\n",
    "    app_index_map = {app: idx for idx, app in enumerate(offline_apps)}\n",
    "    \n",
    "    num_cpu = len(cpu_power_levels)\n",
    "    num_gpu = len(gpu_power_levels)\n",
    "    num_apps = len(offline_apps)\n",
    "    \n",
    "    tensor_ips = np.full((num_cpu, num_gpu, num_apps), np.nan)\n",
    "    tensor_flops = np.full((num_cpu, num_gpu, num_apps), np.nan)\n",
    "    \n",
    "    for _, row in df_full.iterrows():\n",
    "        cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "        gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "        app_idx = app_index_map[row[\"App\"]]\n",
    "        tensor_ips[cpu_idx, gpu_idx, app_idx] = row[\"IPS\"]\n",
    "        tensor_flops[cpu_idx, gpu_idx, app_idx] = row[\"FLOPS\"]\n",
    "    \n",
    "    ips_max = np.nanmax(tensor_ips)\n",
    "    flops_max = np.nanmax(tensor_flops)\n",
    "    \n",
    "    tensor_ips /= ips_max\n",
    "    tensor_flops /= flops_max\n",
    "    \n",
    "    for file in online_csv_files:\n",
    "        app_name = file.replace(\"_performance.csv\", \"\")\n",
    "        df_online = pd.read_csv(os.path.join(online_dir, file))\n",
    "        df_online[\"App\"] = app_name\n",
    "    \n",
    "        tensor_ips = np.pad(tensor_ips, ((0, 0), (0, 0), (0, 1)), constant_values=np.nan)\n",
    "        tensor_flops = np.pad(tensor_flops, ((0, 0), (0, 0), (0, 1)), constant_values=np.nan)\n",
    "        app_index_map[app_name] = tensor_ips.shape[2] - 1  \n",
    "    \n",
    "        df_known = df_online[df_online[[\"CPU Power Cap\", \"GPU Power Cap\"]].apply(tuple, axis=1).isin(selected_combinations)]\n",
    "        df_test = df_online.drop(df_known.index)\n",
    "        \n",
    "        for _, row in df_known.iterrows():\n",
    "            cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "            gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "            app_idx = app_index_map[row[\"App\"]]\n",
    "            tensor_ips[cpu_idx, gpu_idx, app_idx] = row[\"IPS\"] / ips_max\n",
    "            tensor_flops[cpu_idx, gpu_idx, app_idx] = row[\"FLOPS\"] / flops_max\n",
    "\n",
    "        def svd_impute(tensor):\n",
    "            nan_mask = np.isnan(tensor)\n",
    "            tensor_2d = tensor.reshape(-1, tensor.shape[-1])\n",
    "            \n",
    "            imputer = SimpleImputer(strategy=\"mean\")\n",
    "            tensor_2d_filled = imputer.fit_transform(tensor_2d)\n",
    "            \n",
    "            svd = TruncatedSVD(n_components=min(3, tensor_2d_filled.shape[1]-1))\n",
    "            low_rank_approx = svd.fit_transform(tensor_2d_filled)\n",
    "            tensor_filled = svd.inverse_transform(low_rank_approx)\n",
    "            \n",
    "            return tensor_filled.reshape(tensor.shape)\n",
    "        \n",
    "        tensor_ips_filled = svd_impute(tensor_ips)\n",
    "        tensor_flops_filled = svd_impute(tensor_flops)\n",
    "        \n",
    "        ranks = [min(10, tensor_ips.shape[0]), min(10, tensor_ips.shape[1]), min(10, tensor_ips.shape[2])]\n",
    "        core_ips, factors_ips = tucker(tensor_ips_filled, rank=ranks)\n",
    "        completed_ips = tl.tucker_to_tensor((core_ips, factors_ips))\n",
    "\n",
    "        core_flops, factors_flops = tucker(tensor_flops_filled, rank=ranks)\n",
    "        completed_flops = tl.tucker_to_tensor((core_flops, factors_flops))\n",
    "    \n",
    "        test_data_ips = []\n",
    "        test_data_flops = []\n",
    "        \n",
    "        for _, row in df_test.iterrows():\n",
    "            cpu_idx = cpu_index_map[row[\"CPU Power Cap\"]]\n",
    "            gpu_idx = gpu_index_map[row[\"GPU Power Cap\"]]\n",
    "            app_idx = app_index_map[row[\"App\"]]\n",
    "            test_data_ips.append((cpu_idx, gpu_idx, app_idx, row[\"IPS\"] / ips_max))\n",
    "            test_data_flops.append((cpu_idx, gpu_idx, app_idx, row[\"FLOPS\"] / flops_max))\n",
    "    \n",
    "        true_ips, pred_ips = [], []\n",
    "        true_flops, pred_flops = [], []\n",
    "        \n",
    "        for cpu_idx, gpu_idx, app_idx, true_val in test_data_ips:\n",
    "            pred_val = completed_ips[cpu_idx, gpu_idx, app_idx]\n",
    "            true_ips.append(true_val)\n",
    "            pred_ips.append(pred_val)\n",
    "        \n",
    "        for cpu_idx, gpu_idx, app_idx, true_val in test_data_flops:\n",
    "            pred_val = completed_flops[cpu_idx, gpu_idx, app_idx]\n",
    "            true_flops.append(true_val)\n",
    "            pred_flops.append(pred_val)\n",
    "    \n",
    "        mae_ips = mean_absolute_error(true_ips, pred_ips) * ips_max\n",
    "        rmse_ips = np.sqrt(mean_squared_error(true_ips, pred_ips)) * ips_max\n",
    "        r2_ips = r2_score(true_ips, pred_ips)\n",
    "        \n",
    "        mae_flops = mean_absolute_error(true_flops, pred_flops) * flops_max\n",
    "        rmse_flops = np.sqrt(mean_squared_error(true_flops, pred_flops)) * flops_max\n",
    "        r2_flops = r2_score(true_flops, pred_flops)\n",
    "        \n",
    "        RMSE_IPS.append(rmse_ips)\n",
    "        RMSE_FLOPS.append(rmse_flops)\n",
    "    \n",
    "        # print(f\"IPS Prediction Accuracy: MAE={mae_ips:.4f}, RMSE={rmse_ips:.4f}, RÂ²={r2_ips:.4f}\")\n",
    "        # print(f\"FLOPS Prediction Accuracy: MAE={mae_flops:.4f}, RMSE={rmse_flops:.4f}, RÂ²={r2_flops:.4f}\")\n",
    "\n",
    "    return round(float(mean(RMSE_IPS)), 4), round(float(mean(RMSE_FLOPS)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e95c3132-133f-4fda-b1fd-f8acc70b4619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1274"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f16a4bd-16d0-41cc-aa0c-ad078bc90160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest avg_rmse: inf\n",
      "Best Combinations:\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "import shutil\n",
    "\n",
    "# Define directories\n",
    "SOURCE_DIR = Path(\"./altis_power_cap_res\")\n",
    "OFFLINE_DIR = SOURCE_DIR / \"offline\"\n",
    "ONLINE_DIR = SOURCE_DIR / \"online\"\n",
    "\n",
    "# Ensure offline and online directories exist\n",
    "OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ONLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all performance CSV files\n",
    "files = list(SOURCE_DIR.glob(\"*_performance.csv\"))\n",
    "\n",
    "avg_rmse = float('inf')  # Initialize to infinity\n",
    "best_combinations = []  # Store best combinations\n",
    "\n",
    "# Function to move files back to the source directory\n",
    "def move_back():\n",
    "    for file in OFFLINE_DIR.glob(\"*.csv\"):  # Only move CSV files\n",
    "        shutil.move(str(file), str(SOURCE_DIR))\n",
    "    for file in ONLINE_DIR.glob(\"*.csv\"):  # Only move CSV files\n",
    "        shutil.move(str(file), str(SOURCE_DIR))\n",
    "\n",
    "# Generate combinations of 14 files from the total files\n",
    "for combo in itertools.combinations(files, 7):\n",
    "    # Move selected 14 files to offline\n",
    "    for file in combo:\n",
    "        shutil.move(str(file), str(OFFLINE_DIR))\n",
    "\n",
    "    # Move the remaining 2 files to online\n",
    "    remaining_files = list(SOURCE_DIR.glob(\"*_performance.csv\"))\n",
    "    for file in remaining_files:\n",
    "        shutil.move(str(file), str(ONLINE_DIR))\n",
    "\n",
    "    # Compute RMSE for this combination\n",
    "    current_rmse = cf()\n",
    "\n",
    "    # Update best combinations if a new minimum RMSE is found\n",
    "    if current_rmse < avg_rmse:\n",
    "        avg_rmse = current_rmse\n",
    "        best_combinations = [(list(combo), remaining_files)]  # Reset best list\n",
    "    elif current_rmse == avg_rmse:\n",
    "        best_combinations.append((list(combo), remaining_files))  # Store additional best combos\n",
    "\n",
    "    # Move files back to original directory\n",
    "    move_back()\n",
    "\n",
    "# Print the best RMSE and corresponding combinations\n",
    "print(f\"Smallest avg_rmse: {avg_rmse}\")\n",
    "print(\"Best Combinations:\")\n",
    "\n",
    "for i, (best_combo, remaining_files) in enumerate(best_combinations, 1):\n",
    "    print(f\"\\nCombination {i}:\")\n",
    "    print(f\" - Offline (14 files): {[file.name for file in best_combo]}\")\n",
    "    print(f\" - Online (2 files): {[file.name for file in remaining_files]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db49a0a-e6b8-48e5-9892-71feba004536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
